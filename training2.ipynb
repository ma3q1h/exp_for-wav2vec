{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training ver2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import & setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hugging face login\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open('./config.yml', 'rb') as yml:\n",
    "    config = yaml.safe_load(yml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://note.mjunya.com/posts/2021-12-13-multi-gpu-order/\n",
    "import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=config['CUDA_VISIBLE_DEVICES']\n",
    "!echo ${CUDA_VISIBLE_DEVICES}\n",
    "\n",
    "import torch\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    info = torch.cuda.get_device_properties(i)\n",
    "    print(f\"CUDA:{i} {info.name}, {info.total_memory / 1024 ** 2}MB\")\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(f\"version: {torch.__version__}\")\n",
    "print(f\"available: {torch.cuda.is_available()}\")\n",
    "print(f\"count: {torch.cuda.device_count()}\")\n",
    "for i in range(0,torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_capability(i)}\")\n",
    "print(f\"default: {torch.cuda.current_device()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchaudio, datasets, warnings\n",
    "from datasets import load_dataset, load_metric, Audio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name=config['repo_name']\n",
    "target=config['target']\n",
    "lr=config['lr']\n",
    "TRAIN_ALL_WEIGHTS=config['TRAIN_ALL_WEIGHTS']\n",
    "num_train_epochs=config['num_train_epochs']\n",
    "per_device_train_batch_size=config['per_device_train_batch_size']\n",
    "torch.backends.cudnn.benchmark=config['torch.backends.cudnn.benchmark']\n",
    "sr=config['sr']\n",
    "train_csv='./datasets/train_'+target+'.csv'\n",
    "val_csv='./datasets/val_'+target+'.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= datasets.load_dataset(\"csv\", data_files={\"train\":[train_csv]},usecols=['path',target],num_proc=config['num_proc'])\n",
    "val=datasets.load_dataset(\"csv\", data_files={\"train\":[val_csv]},usecols=['path',target],num_proc=config['num_proc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    display(HTML(df.to_html()))\n",
    "    \n",
    "show_random(train['train'],2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make token list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://engineers.ntt.com/entry/2021/12/20/172148\n",
    "\n",
    "def extract_token(batch):\n",
    "  all_label = \" \".join(batch[target])\n",
    "  vocab = list(set(all_label))\n",
    "  return {\"vocab\": [vocab], \"all_text\": [all_label]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_train=train.map(extract_token,batched=True,batch_size=-1,keep_in_memory=True,remove_columns=train.column_names['train'])\n",
    "vocab_val= val.map(extract_token, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=val.column_names['train'])\n",
    "vocab_list= list(set(vocab_train[\"train\"][\"vocab\"][0]) | set(vocab_val[\"train\"][\"vocab\"][0]))\n",
    "vocab_dict= {v: k for k, v in enumerate(vocab_list)}\n",
    "print(len(vocab_dict))\n",
    "vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]\n",
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"pau\"] = len(vocab_dict) # データセット、openjtalkの表記と合わせる必要あり\n",
    "len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f'./token_{target}.json', 'w') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASR setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import Wav2Vec2PhonemeCTCTokenizer\n",
    "#tokenizer = Wav2Vec2PhonemeCTCTokenizer(vocab_file=f'{exp_dir}token_{tgt}.json', unk_token=\"[UNK]\", pad_token=\"pau\",do_phonemize=False, word_delimiter_token=\"|\", phone_delimiter_token=\"|\", phonemizer_lang=\"ja\", phonemizer_backend='espeak')\n",
    "#tokenizer.push_to_hub(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "tokenizer = Wav2Vec2CTCTokenizer(vocab_file=f'./token_{target}.json', unk_token=\"[UNK]\", pad_token=\"pau\", word_delimiter_token=\"|\")\n",
    "tokenizer.push_to_hub(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=sr, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "feature_extractor\n",
    "# (large) return_attention_mask=True (base) False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocess audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path2array(batch):\n",
    "    array, rate = torchaudio.load(filepath=batch['path'],format='wav')\n",
    "    batch[\"audio_array\"]= array\n",
    "    batch[\"sampling_rate\"] =rate\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train=train.map(path2array,num_proc=config['num_proc'])\n",
    "val=val.map(path2array,num_proc=config['num_proc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.remove_columns(['path','sampling_rate'])\n",
    "val=val.remove_columns(['path','sampling_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "# def array_dim(batch):\n",
    "#     batch[\"array_dim\"]=len(batch[\"audio_array\"])\n",
    "#     batch[\"1Darray\"]= list(itertools.chain.from_iterable(batch[\"audio_array\"]))\n",
    "#     batch[\"sample\"]=len(batch[\"1Darray\"])\n",
    "#     return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b=train['train'][3]['audio_array'] #list,1\n",
    "# c=processor(b, sampling_rate=sr)   #transformers.feature_extraction_utils.BatchFeature,2\n",
    "# d=processor(b, sampling_rate=sr).input_values[0] #numpy.ndarray,8661\n",
    "# e=processor(b, sampling_rate=sr).input_values    #list,1\n",
    "# f=processor(b, sampling_rate=sr).input_values[-1] #numpy.ndarray,8661\n",
    "# g=processor(b, sampling_rate=sr).attention_mask #list,1 all=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no change name [\"input_values\"],[\"labels\"]\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    batch[\"input_values\"] = processor(batch[\"audio_array\"], sampling_rate=sr).input_values[0]\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[target]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_prepared=train.map(prepare_dataset,remove_columns=train.column_names[\"train\"],num_proc=config['num_proc'])\n",
    "val_prepared=val.map(prepare_dataset,remove_columns=val.column_names[\"train\"],num_proc=config['num_proc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import jiwer\n",
    "wer_metric = load_metric(\"wer\")\n",
    "cer_metric = load_metric('cer')\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)    \n",
    "\n",
    "    return {\"wer\": wer, \"cer\": cer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    pretrained_model_name_or_path=config['pretrained_name'],\n",
    "    attention_dropout=config['attention_dropout'],\n",
    "    hidden_dropout=config['hidden_dropout'],\n",
    "    feat_proj_dropout=config['feat_proj_dropout'],\n",
    "    mask_time_prob=config['mask_time_prob'],\n",
    "    layerdrop=config['layerdrop'],\n",
    "    ctc_loss_reduction=config['ctc_loss_reduction'], \n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    diversity_loss_weight=config['diversity_loss_weight'],\n",
    "    vocab_size=len(processor.tokenizer),\n",
    "    ignore_mismatched_sizes=config['ignore_mismatched_sizes'],\n",
    ")\n",
    "#model.lm_head = nn.Linear(1024,len(processor.tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_ALL_WEIGHTS:\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "else:\n",
    "    model.freeze_feature_extractor()\n",
    "#model.freeze_feature_extractor()    #jdrtのときはこっち？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_steps = 5 * len(train)//32\n",
    "num_total_steps = num_train_epochs * len(train)//32\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "#input-output\n",
    "  output_dir='./'+repo_name,\n",
    "  logging_dir=\"./\"+\"logs\",\n",
    "  push_to_hub=config['push_to_hub'],\n",
    "  save_total_limit=config['save_total_limit'],\n",
    "  seed=config['seed'],\n",
    "#batch\n",
    "  per_device_train_batch_size=per_device_train_batch_size, \n",
    "  per_device_eval_batch_size=per_device_train_batch_size,\n",
    "  evaluation_strategy=config['evaluation_strategy'],\n",
    "  save_strategy=config['save_strategy'],\n",
    "  logging_steps=config['logging_steps'],\n",
    "  num_train_epochs=num_train_epochs,\n",
    "  #eval_steps=config['eval_steps],\n",
    "#lr\n",
    "  learning_rate=lr,\n",
    "  lr_scheduler_type=config['lr_scheduler_type'],\n",
    "  weight_decay=config['weight_decay'],\n",
    "  warmup_steps=config['warmup_steps'], #この数分学習率増加してから減少させるスケジューラ\n",
    "#tokens\n",
    "  group_by_length=config['group_by_length'],\n",
    "  prediction_loss_only=config['prediction_loss_only'],\n",
    "\n",
    "#faster\n",
    "  dataloader_num_workers=os.cpu_count(),\n",
    "  fp16=config['fp16'],\n",
    "  fp16_full_eval=config['fp16_full_eval'],\n",
    "  gradient_checkpointing=config['gradient_checkpointing'],\n",
    "  gradient_accumulation_steps=32//per_device_train_batch_size,\n",
    ")\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_prepared['train'],\n",
    "    eval_dataset=val_prepared['train'],\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],  #3epoch未改善でearly stop\n",
    ")\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_state()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docker",
   "language": "python",
   "name": "docker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
